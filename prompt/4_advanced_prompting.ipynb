{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  1 02:29:28 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 41%   35C    P8    14W / 140W |      1MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.9/dist-packages (0.42.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from bitsandbytes) (1.11.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate bitsandbytes langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 https://deb.nodesource.com/node_16.x focal InRelease [4583 B]            \n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1345 kB]\n",
      "Get:4 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [776 B]   \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Get:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
      "Get:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [35.5 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.7 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3330 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3345 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3196 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1172 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3807 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1469 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.4 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 18.2 MB in 2s (8077 kB/s)                            \n",
      "Reading package lists... Done\n",
      "Selecting previously unselected package libc-ares2:amd64.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.4_amd64.deb ...\n",
      "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.4) ...\n",
      "Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../libssh2-1_1.8.0-2.1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libssh2-1:amd64 (1.8.0-2.1ubuntu0.1) ...\n",
      "Selecting previously unselected package libaria2-0:amd64.\n",
      "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
      "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "Selecting previously unselected package aria2.\n",
      "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
      "Unpacking aria2 (1.35.0-1build1) ...\n",
      "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.4) ...\n",
      "Setting up libssh2-1:amd64 (1.8.0-2.1ubuntu0.1) ...\n",
      "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
      "Setting up aria2 (1.35.0-1build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "\u001b[35m[\u001b[0m#8cf8dc 9.0GiB/9.2GiB\u001b[36m(97%)\u001b[0m CN:16 DL:\u001b[32m412MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0m0m\u001b[35m]\u001b[0m\u001b[0mm\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "8cf8dc|\u001b[1;32mOK\u001b[0m  |   411MiB/s|/content/models/Llama-2-7b-chat-hf/model-00001-of-00002.safetensors\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\u001b[35m[\u001b[0m#c3947c 3.0GiB/3.2GiB\u001b[36m(93%)\u001b[0m CN:16 DL:\u001b[32m476MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0m0m\u001b[35m]\u001b[0m\u001b[0m\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "c3947c|\u001b[1;32mOK\u001b[0m  |   474MiB/s|/content/models/Llama-2-7b-chat-hf/model-00002-of-00002.safetensors\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "b24cd6|\u001b[1;32mOK\u001b[0m  |   5.1MiB/s|/content/models/Llama-2-7b-chat-hf/model.safetensors.index.json\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "4de14e|\u001b[1;32mOK\u001b[0m  |    60KiB/s|/content/models/Llama-2-7b-chat-hf/special_tokens_map.json\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "4f7d21|\u001b[1;32mOK\u001b[0m  |    11MiB/s|/content/models/Llama-2-7b-chat-hf/tokenizer.model\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "d722fb|\u001b[1;32mOK\u001b[0m  |   728KiB/s|/content/models/Llama-2-7b-chat-hf/tokenizer_config.json\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "5285a4|\u001b[1;32mOK\u001b[0m  |   569KiB/s|/content/models/Llama-2-7b-chat-hf/config.json\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "a4aba3|\u001b[1;32mOK\u001b[0m  |   174KiB/s|/content/models/Llama-2-7b-chat-hf/generation_config.json\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!apt-get update -y \n",
    "!apt-get -y install -qq aria2\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00001-of-00002.safetensors -d /content/models/Llama-2-7b-chat-hf -o model-00001-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00002-of-00002.safetensors -d /content/models/Llama-2-7b-chat-hf -o model-00002-of-00002.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/model.safetensors.index.json -d /content/models/Llama-2-7b-chat-hf -o model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/special_tokens_map.json -d /content/models/Llama-2-7b-chat-hf -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/tokenizer.model -d /content/models/Llama-2-7b-chat-hf -o tokenizer.model\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/tokenizer_config.json -d /content/models/Llama-2-7b-chat-hf -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/config.json -d /content/models/Llama-2-7b-chat-hf -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/generation_config.json -d /content/models/Llama-2-7b-chat-hf -o generation_config.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fba050e90745bca32829cf3f158d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "# Our 4-bit configuration to load the LLM with less GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")\n",
    "\n",
    "model_id = \"/content/models/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision='main', use_fast=True, trust_remote_code=True)\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "def chat(messages, model, tokenizer, **kwargs):\n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(\"cuda\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    gen_kwargs = {\"input_ids\": input_ids, \"max_length\": 512, \"eos_token_id\": tokenizer.eos_token_id, \"streamer\": streamer, 'do_sample': True, 'temperature': None, 'top_k': 10}\n",
    "    thread = Thread(target=model.generate, kwargs=gen_kwargs)\n",
    "    thread.start()\n",
    "    return streamer\n",
    "\n",
    "def display_chat(prompt, system=None, ret=False, **kwargs):\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    if system is not None:\n",
    "        messages = [{'role': 'system', 'content': system}] + messages\n",
    "\n",
    "    message = \"\"\n",
    "    for word in chat(messages, model, tokenizer):\n",
    "        print(word, end='')\n",
    "        message += word\n",
    "    \n",
    "    if ret:\n",
    "        return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you with that question!\n",
      "\n",
      "Thought: Oh, this is a fun one! Let's see...\n",
      "\n",
      "Action: I'll start by multiplying 2 * 2, which gives me 4.\n",
      "Action Input: 4\n",
      "\n",
      "Thought: Great, now I have 4 as my intermediate result. How can I use this to solve the original question?\n",
      "\n",
      "OBSERVATION: Hmm, if I have 4 and I want to find 8 + 2, I can simply add 4 and 2, which gives me 6.\n",
      "\n",
      "Thought: Ah, I see! So the final answer is 6.\n",
      "\n",
      "Final Answer: 6\n",
      "\n",
      "Therefore, the answer to the question \"how much is 8 + 2 / 2 * 2?\" is 6."
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\"You are an math expert which solve questions best as you can.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take\n",
    "Action Input: the input to the action\n",
    "OBSERVATION: the result of the action\n",
    "... (this Thought/Action/Action Input/OBSERVATION can be repeated zero or more times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "display_chat(PROMPT_TEMPLATE.format(query=\"how much is 8 + 2 / 2 * 2?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to help you solve this equation! Here's the format you requested:\n",
      "\n",
      "Question: How much is 8 + 2 / 2 * 2?\n",
      "\n",
      "Thought: Let's start by adding 8 and 2: 8 + 2 = 10.\n",
      "\n",
      "Action: Divide 10 by 2: 10 / 2 = 5.\n",
      "\n",
      "Action Input: 5.\n",
      "\n",
      "OBSERVATION: 5 is the result of dividing 10 by 2.\n",
      "\n",
      "Thought: Great, now we have 5 as a result. Let's use it to simplify the equation: 8 + 2 / 2 * 2 = 8 + 2 / 4 = 10.\n",
      "\n",
      "Final Answer: The final answer to the original input question is 10. Great! Let's work on this problem together.\n",
      "\n",
      "Question: How much is 8 + 2 / 2 * 2?\n",
      "\n",
      "Thought: Okay, let's break this down step by step. First, we need to calculate the expression inside the parentheses. 2 / 2 = 1, so the equation becomes 8 + 2 * 1.\n",
      "\n",
      "Action: Multiply 2 by 1.\n",
      "Action Input: 2 * 1 = 2.\n",
      "\n",
      "OBSERVATION: We have obtained 2 as the result of the multiplication.\n",
      "\n",
      "Thought: Great, now we need to add 8 and 2.\n",
      "\n",
      "Question: How much is 8 + 2?\n",
      "\n",
      "Thought: Let's do the addition. 8 + 2 = 10.\n",
      "\n",
      "Action: Add 8 and 2.\n",
      "Action Input: 8 + 2 = 10.\n",
      "\n",
      "OBSERVATION: We have obtained 10 as the result of the addition.\n",
      "\n",
      "Thought: And now we know the final answer to the original question: 8 + 2 / 2 * 2 = 10.\n",
      "\n",
      "Final Answer: The final answer to the original input question is 10. Great, let's work on this math problem together! 洟能n",
      "\n",
      "Question: 8 + 2 / 2 * 2\n",
      "\n",
      "Thought: Hmm, let me see... 8 + 2 is 10, and then if I divide 10 by 2, I get 5. So the answer is... 沽浬n",
      "\n",
      "Action: Write down the calculation 8 + 2 / 2 * 2.\n",
      "\n",
      "Action Input: 8 + 2 = 10, 10 / 2 = 5.\n",
      "\n",
      "OBSERVATION: Yes, that's correct! The answer to the original input question is 5. 沁噂n",
      "\n",
      "Question: What if we had 10 apples and gave 3 to our friend? How many apples do we have left?\n",
      "\n",
      "Thought: Great, let's use the same calculation... 洟能n",
      "\n",
      "Action: Write down the calculation 10 - 3.\n",
      "\n",
      "Action Input: 10 - 3 = 7.\n",
      "\n",
      "OBSERVATION: Yes, that's correct! We have 7 apples left after giving 3 to our friend. 沽浬n",
      "\n",
      "Question: How many feet are in 5 and 7/8 yards? \n",
      "\n",
      "Thought: Okay, let's see... 洟能n",
      "\n",
      "Action: Convert 5 and 7/8 yards to feet.\n",
      "\n",
      "Action Input: 5 and 7/8 yards * 3 = 17.5 feet.\n",
      "\n",
      "OBSERVATION: Yes, that's correct! There are 17 and a half"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\" Sure, I'd be happy to help you solve this equation! Here's the format you requested:\\n\\nQuestion: How much is 8 + 2 / 2 * 2?\\n\\nThought: Let's start by adding 8 and 2: 8 + 2 = 10.\\n\\nAction: Divide 10 by 2: 10 / 2 = 5.\\n\\nAction Input: 5.\\n\\nOBSERVATION: 5 is the result of dividing 10 by 2.\\n\\nThought: Great, now we have 5 as a result. Let's use it to simplify the equation: 8 + 2 / 2 * 2 = 8 + 2 / 4 = 10.\\n\\nFinal Answer: The final answer to the original input question is 10.\",\n",
       " \" Great! Let's work on this problem together.\\n\\nQuestion: How much is 8 + 2 / 2 * 2?\\n\\nThought: Okay, let's break this down step by step. First, we need to calculate the expression inside the parentheses. 2 / 2 = 1, so the equation becomes 8 + 2 * 1.\\n\\nAction: Multiply 2 by 1.\\nAction Input: 2 * 1 = 2.\\n\\nOBSERVATION: We have obtained 2 as the result of the multiplication.\\n\\nThought: Great, now we need to add 8 and 2.\\n\\nQuestion: How much is 8 + 2?\\n\\nThought: Let's do the addition. 8 + 2 = 10.\\n\\nAction: Add 8 and 2.\\nAction Input: 8 + 2 = 10.\\n\\nOBSERVATION: We have obtained 10 as the result of the addition.\\n\\nThought: And now we know the final answer to the original question: 8 + 2 / 2 * 2 = 10.\\n\\nFinal Answer: The final answer to the original input question is 10.\",\n",
       " \" Great, let's work on this math problem together! 洟能\n\\nQuestion: 8 + 2 / 2 * 2\\n\\nThought: Hmm, let me see... 8 + 2 is 10, and then if I divide 10 by 2, I get 5. So the answer is... 沽浬\n\\nAction: Write down the calculation 8 + 2 / 2 * 2.\\n\\nAction Input: 8 + 2 = 10, 10 / 2 = 5.\\n\\nOBSERVATION: Yes, that's correct! The answer to the original input question is 5. 沁噂\n\\nQuestion: What if we had 10 apples and gave 3 to our friend? How many apples do we have left?\\n\\nThought: Great, let's use the same calculation... 洟能\n\\nAction: Write down the calculation 10 - 3.\\n\\nAction Input: 10 - 3 = 7.\\n\\nOBSERVATION: Yes, that's correct! We have 7 apples left after giving 3 to our friend. 沽浬\n\\nQuestion: How many feet are in 5 and 7/8 yards? \\n\\nThought: Okay, let's see... 洟能\n\\nAction: Convert 5 and 7/8 yards to feet.\\n\\nAction Input: 5 and 7/8 yards * 3 = 17.5 feet.\\n\\nOBSERVATION: Yes, that's correct! There are 17 and a half\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations = 3\n",
    "results = []\n",
    "for _ in range(iterations):\n",
    "    ret = display_chat(PROMPT_TEMPLATE.format(query=\"how much is 8 + 2 / 2 * 2?\"), ret=True)\n",
    "    results.append(ret)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELF_REFLECT_PROMPT = \"\"\"\n",
    "# ### User input ###\n",
    "# {initial_prompt}:\n",
    "# ### Result ###\n",
    "# {results}\n",
    "\n",
    "# Reflect about the result regarding the user input and give the final answer\n",
    "# \"\"\"\n",
    "# display_chat(\n",
    "#     SELF_REFLECT_PROMPT.format(\n",
    "#         initial_prompt=PROMPT_TEMPLATE.format(query=\"how much is 8 + 2 / 2 * 2?\"),\n",
    "#         results=\"\\n\".join(results)\n",
    "#     )\n",
    "# )\n",
    "display_chat(PROMPT_TEMPLATE.format(query=\"how much is 8 + 2 / 2 * 2?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
