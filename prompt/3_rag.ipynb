{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 17 04:50:27 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 33%   27C    P0    41W / 230W |      1MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63588c0e74384dff828591cfb9d64d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "# Our 4-bit configuration to load the LLM with less GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")\n",
    "\n",
    "model_id = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, revision='main', use_fast=True, trust_remote_code=True)\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to('cuda') for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task='text-generation',\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_links = [\n",
    "    \"https://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse\",\n",
    "    \"https://cloud.google.com/architecture/data-mesh\",\n",
    "    \"https://cloud.google.com/architecture/design-self-service-data-platform-data-mesh\",\n",
    "    \"https://cloud.google.com/architecture/describe-organize-data-products-resources-data-mesh\",\n",
    "    \"https://cloud.google.com/architecture/build-data-products-data-mesh\",\n",
    "    \"https://cloud.google.com/architecture/discover-consume-data-products-data-mesh\",\n",
    "    \"https://cloud.google.com/architecture/big-data-analytics/data-warehouse\",\n",
    "    \"https://cloud.google.com/architecture/marketing-data-warehouse-on-gcp\",\n",
    "    \"https://cloud.google.com/architecture/cicd-pipeline-for-data-processing\",\n",
    "    \"https://cloud.google.com/architecture/cicd-pipeline-for-data-processing/deployment\",\n",
    "    \"https://cloud.google.com/architecture/automatically-apply-sensitivity-tags-in-data-catalog\",\n",
    "    \"https://cloud.google.com/architecture/partners/building-custom-data-integrations-using-fivetran-and-cloud-functions\",\n",
    "    \"https://cloud.google.com/architecture/partners/continuous-data-replication-bigquery-striim\",\n",
    "    \"https://cloud.google.com/architecture/ingesting-clinical-and-operational-data-with-cloud-data-fusion\",\n",
    "    \"https://cloud.google.com/architecture/performing-etl-from-relational-database-into-bigquery\",\n",
    "    \"https://cloud.google.com/architecture/tracking-provenance-and-lineage-metadata-for-healthcare-data\",\n",
    "    \"https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc\",\n",
    "    \"https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc/deployment\",\n",
    "    \"https://cloud.google.com/architecture/partners/using-fivetran-and-elt-with-bigquery\",\n",
    "    \"https://cloud.google.com/architecture/data-pipeline-mongodb-gcp\",\n",
    "    \"https://cloud.google.com/architecture/data-pipeline-mongodb-gcp/deployment\",\n",
    "    \"https://cloud.google.com/architecture/analyzing-fhir-data-in-bigquery\",\n",
    "    \"https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker\",\n",
    "    \"https://cloud.google.com/architecture/reference-patterns/overview\",\n",
    "    \"https://cloud.google.com/architecture/data-science-with-r-on-gcp-eda\",\n",
    "    \"https://cloud.google.com/architecture/genomic-data-processing-reference-architecture\",\n",
    "    \"https://cloud.google.com/architecture/geospatial-analytics-architecture\",\n",
    "    \"https://cloud.google.com/architecture/propensity-modeling-gaming\",\n",
    "    \"https://cloud.google.com/architecture/set-up-regulatory-reporting-architecture-bigquery\",\n",
    "    \"https://cloud.google.com/architecture/build-smart-api-predict-customer-purchase-apigee-bigquery-ml-cloud-spanner\",\n",
    "    \"https://cloud.google.com/architecture/build-smart-api-predict-customer-purchase-apigee-bigquery-ml-cloud-spanner/deployment\",\n",
    "] \n",
    "\n",
    "loader = WebBaseLoader(web_links)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You need to be familiar with the concepts described in the series, including architecture and functions in a data mesh, designing a self-service data platform, describing and organizing data products and resources, building data products, and discovering and consuming data products. Additionally, you should understand the key terms used in the architecture, such as data products, data governance standards, and distributed teams.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What I need to implement a data mesh architecture?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Google Cloud Dataflow is a fully managed service that makes it easy to process and analyze large amounts of data using Apache Beam. With Dataflow, you can create powerful data processing pipelines that can handle complex tasks like data transformation, filtering, and aggregation. Additionally, Dataflow provides built-in support for data validation, monitoring, and security, making it a great choice for organizations looking to streamline their data processing workflows. Would you like to learn more about how Dataflow can help with your organization's data processing needs?\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "\n",
    "query = \"What GCP products we need?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"What's next\\n\\nFind Google Dataflow Templates on\\nGitHub \\nif you want to customize the templates.\\nLearn more about MongoDB Atlas and Google Cloud solutions on\\nCloud skill boost.\\nLearn more about the Google Cloud products used in this reference\\narchitecture:\\n\\nBigQuery \\nPub/Sub \\nDataflow \\nCompute Engine \\n\\n\\n\\nFor more reference architectures, diagrams, and best practices, explore the\\nCloud Architecture Center.\\n\\n\\n\\nContributors\\nAuthors:\\n\\nSaurabh Kumar | ISV\\nPartner Engineer\\nVenkatesh Shanbhag | Senior\\nSolutions Architect (MongoDB)\\n\\nOther contributors:\\n\\nJun Liu | Supportability Tech\\nLead\\nMaridu Raju Makaraju | Supportability\\nTech Lead\\nSergei Lilichenko | Solutions\\nArchitect\\nShan Kulandaivel | Group Product Manager\\n\\nTo see nonpublic LinkedIn profiles, sign in to LinkedIn.\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\", metadata={'source': 'https://cloud.google.com/architecture/data-pipeline-mongodb-gcp/deployment', 'title': 'Deploy a data transformation process between MongoDB Atlas and Google Cloud \\xa0|\\xa0 Cloud Architecture Center', 'description': 'Data transformation between Mongo DB Atlas as the operational data store and BigQuery as the analytics data warehouse.', 'language': 'en'}),\n",
      " Document(page_content=\"Workflows:\\nA fully managed orchestration platform that executes services in a\\nspecified order as a workflow. Workflows can combine services, including\\ncustom services hosted on Cloud Run or Cloud Functions,\\nGoogle Cloud services such as BigQuery, and any\\nHTTP-based API.\\n\\nArchitecture\\nThe example lakehouse architecture that this solution deploys analyzes\\nan ecommerce dataset to understand a retailer's performance over time. The\\nfollowing diagram shows the architecture of the Google Cloud resources\\nthat the solution deploys.\\n\\nSolution flow\\nThe architecture represents a common data flow to populate and transform data\\nin an analytics lakehouse architecture:\", metadata={'source': 'https://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse', 'title': 'Jump Start Solution: Analytics lakehouse \\xa0|\\xa0 Cloud Architecture Center \\xa0|\\xa0 Google Cloud', 'description': 'Helps you understand and deploy the Analytics lakehouse Jump Start Solution.', 'language': 'en'}),\n",
      " Document(page_content=\"connections.\\nThis document is intended for a technical audience whose responsibilities\\ninclude data security, data governance, data processing, or data analytics. This\\ndocument assumes that you're familiar with data processing and data privacy,\\nwithout being an expert. It also assumes that you have some familiarity with shell\\nscripts and a basic knowledge of Google Cloud.\\nArchitecture\\nThis architecture defines a pipeline that performs the following actions:\", metadata={'source': 'https://cloud.google.com/architecture/automatically-apply-sensitivity-tags-in-data-catalog', 'title': 'Automatically apply sensitivity tags in Data Catalog to files, databases, and BigQuery tables using Sensitive Data Protection and Dataflow \\xa0|\\xa0 Cloud Architecture Center \\xa0|\\xa0 Google Cloud', 'description': 'Shows how to use Data Catalog with an automated Dataflow pipeline to identify and apply data sensitivity tags to your data in Cloud Storage files, relational databases, and BigQuery.', 'language': 'en'}),\n",
      " Document(page_content='The following Google Cloud products are used to stage data in the\\nsolution for first use:\\n\\nWorkflows:\\nA fully managed orchestration platform that executes services in a\\nspecified order as a workflow. Workflows can combine services, including\\ncustom services hosted on Cloud Run or Cloud Functions,\\nGoogle Cloud services such as BigQuery, and any\\nHTTP-based API.\\nPub/Sub:\\nAn asynchronous and scalable messaging service that decouples services that\\nproduce messages from services that process those messages.\\nEventarc:\\nA service that manages the flow of state changes (events) between decoupled\\nmicroservices, routing events to various destinations while managing\\ndelivery, security, authorization, observability, and error handling.\\n\\nArchitecture\\nThe example warehouse that this solution deploys analyzes fictional ecommerce\\ndata from TheLook\\nto understand company performance over time. The following diagram shows the\\narchitecture of the Google Cloud resources that the solution deploys.', metadata={'source': 'https://cloud.google.com/architecture/big-data-analytics/data-warehouse', 'title': 'Jump Start Solution: Data warehouse with BigQuery \\xa0|\\xa0 Cloud Architecture Center \\xa0|\\xa0 Google Cloud', 'description': 'Helps you understand and deploy the data warehouse with moonrise-replace98500fb0df454df6adfe87c740b4441dmoonrise-replace Jump Start Solution.', 'language': 'en'})]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(result['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
